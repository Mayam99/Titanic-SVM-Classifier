# -*- coding: utf-8 -*-
"""Titanic(SVM)-Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QKHEtOrm24cfSEpygpcbGHiYplKGHK_g

##Introduction

###This project explores the application of the Support Vector Machine (SVM) algorithm to the Titanic dataset, one of the most popular datasets available on Kaggle. The Titanic dataset contains information on the passengers aboard the RMS Titanic, which tragically sank on April 15, 1912. The dataset includes features such as passenger demographics, class, fare, and other relevant attributes that may contribute to understanding the likelihood of passenger survival.

###The main objective of this analysis is to apply the SVM algorithm for classification, aiming to predict whether a passenger survived or not based on the available features. SVM is particularly effective in scenarios where the dataset may contain complex patterns, as it finds an optimal hyperplane to distinguish between classes with maximum margin. This project will demonstrate the full data science workflow, including data cleaning, exploratory data analysis (EDA), feature engineering, model training, and evaluation of the SVM model.
"""

#Importing Necessary Libraries
import pandas as pd #It imports the pandas library and assigns it the alias "pd" for easier use.
import numpy as np #It brings in a tool called numpy, nicknamed "np", to help with number crunching in your code.
from sklearn import datasets #Imports tools to load and use built-in or fetched datasets for machine learning tasks.
from sklearn.model_selection import train_test_split #Imports the function to split data into random train and test subsets for model evaluation.
from sklearn.preprocessing import StandardScaler #Imports the tool to standardize features by removing the mean and scaling to unit variance.
from sklearn.svm import SVC #Imports the Support Vector Classification model for building classification models.
from sklearn.metrics import accuracy_score #Imports the tool to calculate the accuracy of a classification model.

df2 = pd.read_csv("train.csv") #Loads the data from the CSV file "train.csv" into a pandas DataFrame named df2.

df2.head() #Displays the first 5 rows of the DataFrame df2.

#Summarizing the Dataset
print(df2.head())  # Displaying the first 5 rows of the dataset
print(df2.describe())  # Summary statistics of the dataset

df2.fillna({'Embarked': 'mode()[0]'}, inplace=True) #Attempts to fill missing values in the 'Embarked' column with the mode.

# converts the categorical columns 'Sex' and 'Embarked' into numerical representations using one-hot encoding, removing the first category to avoid redundancy.
df2 = pd.get_dummies(df2, columns=['Sex', 'Embarked'], drop_first=True)

# Drop irrelevant columns
df2.drop(columns=['Name', 'Ticket', 'PassengerId'], inplace=True)

# Drop irrelevant columns
df2.drop(columns=['Cabin'], inplace=True)

df2.isnull().sum() #Calculates and displays the number of missing values in each column of the DataFrame df2.

"""###There is 177 missing values in the Age column.

"""

# Impute missing values in the 'Age' column with the mean
mean_age = df2['Age'].mean()
df2.fillna({'Age' : 'mean_age'}, inplace=True)

df2.isnull().sum() #Calculates and displays the number of missing values in each column of the DataFrame df2.

"""###Now we can see that there is no missing value in our dataframe."""

#Segregating the Dataset into Input (X) and Output (y)
X = df2.drop('Survived', axis=1)
y = df2['Survived']

X

y

import matplotlib.pyplot as plt #Import libraries for creating visualizations, aliased as plt.
import seaborn as sns #Import libraries for creating visualizations, aliased as sns.
# Check target variable balance
sns.countplot(x='Survived', data=df2)
plt.title('Survival Count')
plt.show()

# Example of visualizing survival based on a feature
sns.barplot(x='Sex_male', y='Survived', data=df2)
plt.title('Survival Rate by Sex')
plt.show()

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
#Those two lines together standardize the features in the DataFrame X using StandardScaler and store the result in X_scaled.

# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve

# Training the initial SVM model
svc = SVC()
svc.fit(X_train, y_train)

# Evaluate on test data
y_pred = svc.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

X_train.shape, X_test.shape, y_train.shape, y_test.shape #Displays the dimensions (number of rows and columns) of the training and testing datasets.

#Loading the Model (SVM in this case)
model = SVC(kernel='linear', random_state=42)

#Training the Model
model.fit(X_train, y_train)

# Predicting the Result using the Trained Model
y_pred = model.predict(X_test)

y_pred

# Calculating the Accuracy of the Trained Model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the SVM model: {accuracy * 100:.2f}%")

# Predicting the Output of Single Sample using the Trained Model
# Let's use the first sample from the test data as an example
sample = X_test[0].reshape(1, -1)
predicted_class = model.predict(sample)
print(f"Predicted class for the sample: {predicted_class[0]}")
print(f"Actual class for the sample: {y_test.values[0]}")

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
# Define the parameter grid
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],
    'kernel': ['linear', 'rbf', 'poly']
}

# Perform Grid Search
grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', verbose=3)
grid_search.fit(X_train, y_train)

# Get the best parameters and best estimator
print("Best Parameters:", grid_search.best_params_)
best_svc = grid_search.best_estimator_

# Train the model with best parameters
best_svc.fit(X_train, y_train)

# Predict and evaluate on the test set
y_pred_tuned = best_svc.predict(X_test)
print("Tuned Model Accuracy:", accuracy_score(y_test, y_pred_tuned))
print(classification_report(y_test, y_pred_tuned))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_tuned)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
y_pred_proba = best_svc.decision_function(X_test)
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
plt.plot(fpr, tpr, marker='.')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()
print("ROC AUC Score:", roc_auc_score(y_test, y_pred_proba))

# Cross-validation on the best model
cv_scores = cross_val_score(best_svc, X_scaled, y, cv=5)
print("Cross-validation Scores:", cv_scores)
print("Mean Cross-validation Score:", np.mean(cv_scores))

